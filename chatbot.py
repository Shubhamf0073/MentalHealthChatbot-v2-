# -*- coding: utf-8 -*-
"""Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B98c7hkUh53dduxHwmQE669uF6Ibro_x
"""

# from google.colab import drive
# drive.mount('/content/drive')

import os
import faiss
import json
import torch
import pinecone
import numpy as np
import pandas as pd
import seaborn as sns
import streamlit as st
import matplotlib.pyplot as plt
from transformers import pipeline
from transformers import AutoTokenizer
from pinecone import Pinecone, ServerlessSpec
from transformers import pipeline as hf_pipeline
from sentence_transformers import SentenceTransformer


data = pd.read_csv("ESConv.csv")

data.head()

data.info()

data.isnull().sum()

data["emotion_type"].value_counts()

sns.countplot(data = data, x = "emotion_type")
plt.xticks(rotation = 45)
plt.title("Distribution of Emotion Types")
plt.show()

data["content_length"] = data["content"].apply(lambda x: len(str(x).split()))

data[["content", "content_length"]].head()

sns.histplot(data["content_length"], bins = 20)
plt.title("Distribution of Content length (Word Count)")
plt.xlabel("Word count")
plt.show()

pd.to_numeric(data["feedback"])

sns.histplot(data["feedback"].dropna(), bins = 10)
plt.title("Feedback Score Distribution")
plt.show()

sns.boxplot(x = "emotion_type", y = "feedback", data = data)
plt.xticks(rotation = 45)
plt.title("Feedback Scores by Emotion Type")
plt.show()

def clean_text(text):
    text = str(text).strip()
    text = text.replace("\n", " ")
    return text

for col in ["situation", "content"]:

    data[col] = data[col].apply(clean_text)

data["content"].head()

json_path = "FailedESConv.json"

with open(json_path) as f:
    Data = json.load(f)

def combine_conversation(conv):
    context = f"Experience: {conv['experience_type']} | Emotion: {conv['emotion_type']} | Problem: {conv['problem_type']} \n"
    context += f"Situation: {conv['situation']}\nDialogue:\n"
    for turn in conv['dialog']:
        role = turn['speaker'].upper()
        # Optionally add strategy or feedback for listener turns:
        extra = ""
        if role == "LISTENER":
            if 'strategy' in turn['annotation']:
                extra += f" [Strategy: {turn['annotation']['strategy']}]"
            if 'feedback' in turn['annotation']:
                extra += f" [Feedback: {turn['annotation']['feedback']}]"
        context += f"[{role}]{extra}: {turn['content'].strip()}\n"
    return context

conversations = [combine_conversation(conv) for conv in Data]

# !pip install sentence-transformers

model = SentenceTransformer("all-MiniLM-L6-v2")

conversation_embeddings = model.encode(conversations, show_progress_bar = True)
np.save("conversation_embeddings.npy", conversation_embeddings)

# !pip install faiss-cpu

conversation_embeddings = np.load("conversation_embeddings.npy")

dimension = conversation_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(conversation_embeddings)

print(f"FAISS index contains {index.ntotal} conversations")

def retrive_context(query, top_k = 1):
    query_embedding = model.encode([query])
    distances, indices = index.search(query_embedding, top_k)

    return [conversations[idx] for idx in indices[0]]

# !pip install pinecone-client
pc = Pinecone(api_key = "pcsk_7UjC6j_G3F7vu1GjfD9MQpNNFmkSBieGQBmfsK29JqyvZK23aimzbzG1AKrSbf9aefGeaa")

index_name = "emotional-support"
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name = index_name,
        dimension = dimension,
        metric = "euclidean",
        spec = ServerlessSpec(
            cloud = "aws",
            region = "us-east-1"
        )
    )

index = pc.Index(index_name)

ids = [f"conv_{i}" for i in range(len(conversation_embeddings))]
vectors = list(zip(ids, conversation_embeddings.tolist()))
index.upsert(vectors)

def retreive_context(query, top_k = 1):
    query_embedding = model.encode([query]).tolist()
    result = index.query(query_embedding, top_k = top_k, include_metadata = True)

    return [match["id"] for match in result["matches"]]

# !pip install transformers

emotional_model = pipeline("text-classification", model = "j-hartmann/emotion-english-distilroberta-base")

def get_emotion(text):
    """Detects emotional state of the input."""
    result = emotional_model(text)
    return result[0]["label"]

def retrieve_context(query, emotion, top_k=1):
    """
    Retrieve conversation context that matches the query and the detected emotion,
    using the metadata present in the conversations. The conversations variable
    is assumed to be a list of strings that include metadata from your ESConv.csv.
    """
    # Step 1: Filter conversations by emotion.
    filtered_conversations = [
        conv for conv in conversations if f"Emotion: {emotion}" in conv
    ]

    # Fallback: if no conversations match the emotion, use all conversations.
    if not filtered_conversations:
        filtered_conversations = conversations

    # Step 2: Encode the filtered conversations.
    filtered_embeddings = model.encode(filtered_conversations)
    query_embedding = model.encode([query])

    # Step 3: Build a temporary FAISS index with the filtered embeddings.
    dimension = filtered_embeddings.shape[1]
    temp_index = faiss.IndexFlatL2(dimension)
    temp_index.add(np.array(filtered_embeddings))

    # Step 4: Retrieve the most similar contexts.
    distances, indices = temp_index.search(query_embedding, top_k)
    retrieved = [filtered_conversations[idx] for idx in indices[0]]

    return "\n---\n".join(retrieved)

tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neo-2.7B")

def trim_prompt(prompt, max_tokens=512):
    tokens = tokenizer(prompt, return_tensors="pt")["input_ids"][0]
    if len(tokens) > max_tokens:
        tokens = tokens[-max_tokens:]
    return tokenizer.decode(tokens, skip_special_tokens=True)

summarizer = hf_pipeline("summarization", model="facebook/bart-large-cnn")

def summarize_context(context, max_length=50):
    summary = summarizer(context, max_length=max_length, min_length=25, do_sample=False)
    return summary[0]['summary_text']

def generate_response(context, query, emotion):
    # Summarize the retrieved context as concisely as possible.
    context_summary = summarize_context(context, max_length=20)  # Try with a very short summary

    # Build a streamlined prompt.
    prompt = (
        f"{context_summary}\n"
        f"{query}\n"
        f"{emotion}\n\n"
        "Generate a concise, empathetic response (2-3 sentences) that validates the user's feelings and offers supportive advice.\n"
        "Answer:"
    )

    # Optionally trim the prompt if necessary.
    prompt = trim_prompt(prompt, max_tokens=512)

    response = generator(
        prompt,
        max_new_tokens=100,
        num_return_sequences=1,
        truncation=True,
        temperature=0.95,
        top_p=0.9
    )

    # Post-process to remove any echoed prompt text.
    generated_text = response[0]["generated_text"]
    # If "Answer:" is echoed, remove everything before it.
    if "Answer:" in generated_text:
        generated_text = generated_text.split("Answer:", 1)[-1].strip()

    return generated_text

def collect_feedback(response, user_rating):
    """
    Stores the response along with the user feedback for future analysis
    """
    feedback_data = {
        "response": response,
        "user_rating": user_rating
    }

    with open("feedback.json", "a") as f:
        json.dump(feedback_data, f)
        f.write("\n")

#!pip install --upgrade transformers huggingface_hub

generator = pipeline(
    "text-generation",
    model="EleutherAI/gpt-neo-2.7B",
    torch_dtype=torch.float16
)

# Initialize the summarizer model (choose one that fits your use case)
summarizer = hf_pipeline("summarization", model="facebook/bart-large-cnn")

def summarize_context(context, max_length=50):
    summary = summarizer(context, max_length=max_length, min_length=25, do_sample=False)
    return summary[0]['summary_text']

def process_query(query, user_rating=None):
    # Step 1: Emotion Recognition Agent
    emotion = get_emotion(query)
    print(f"[ERA] Detected Emotion: {emotion}")

    # Step 2: Retrieve full context from your dataset
    context = retrieve_context(query, emotion, top_k=1)
    print(f"[CRA] Retrieved Context:\n{context}")

    # Step 3: Summarize the retrieved context
    context_summary = summarize_context(context, max_length=50)
    print(f"[Summary] Context Summary: {context_summary}")

    # Step 4: Response Generation Agent using the summarized context
    prompt = (
        f"Context: {context_summary}\n"
        f"User Query: {query}\n"
        f"Emotion: {emotion}\n"
        "Generate a 2-3 sentence empathetic response that validates the user's feelings and offers supportive advice."
    )

    # Optionally trim the prompt if it's too long (using your trim_prompt function)
    prompt = trim_prompt(prompt, max_tokens=512)

    response = generator(
        prompt,
        max_new_tokens=100,
        num_return_sequences=1,
        truncation=True,
        temperature=0.9,
        top_p=0.95
    )

    final_response = response[0]["generated_text"]
    print(f"[RGA] Generated Response:\n{final_response}")

    # Step 5: Learning Agent - Collect feedback if provided.
    if user_rating is not None:
        collect_feedback(final_response, user_rating)
        print("[LA] Feedback collected.")

    return final_response

# Example usage:
response = process_query("I'm feeling really low today.")
print("\nFinal Response to User:")
print(response)

# !pip install streamlit pyngrok
# !pip install pinecone-client


# Initialize pipelines
emotional_model = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
generator = pipeline(
    "text-generation", model="EleutherAI/gpt-neo-2.7B", torch_dtype=torch.float16
)

# Initialize Pinecone
pinecone.init(api_key="YOUR_PINECONE_API_KEY", environment="YOUR_ENVIRONMENT")
index = pinecone.Index("emotional-support")

# Functions

def get_emotion(text):
    result = emotional_model(text)
    return result[0]["label"]

def retrieve_context(query, emotion, top_k=1):
    query_embedding = model.encode([query]).tolist()
    result = index.query(query_embedding, top_k=top_k, include_metadata=True)
    return [match["metadata"]["text"] for match in result["matches"]]

def summarize_context(context, max_length=50):
    summary = summarizer(context, max_length=max_length, min_length=25, do_sample=False)
    return summary[0]['summary_text']

def trim_prompt(prompt, max_tokens=512):
    tokens = tokenizer(prompt, return_tensors="pt")["input_ids"][0]
    if len(tokens) > max_tokens:
        tokens = tokens[-max_tokens:]
    return tokenizer.decode(tokens, skip_special_tokens=True)

def generate_response(context, query, emotion):
    context_summary = summarize_context(context, max_length=20)
    prompt = (
        f"{context_summary}\n"
        f"{query}\n"
        f"{emotion}\n\n"
        "Generate a concise, empathetic response (2-3 sentences) that validates the user's feelings and offers supportive advice.\n"
        "Answer:"
    )
    prompt = trim_prompt(prompt, max_tokens=512)
    response = generator(
        prompt,
        max_new_tokens=100,
        num_return_sequences=1,
        truncation=True,
        temperature=0.95,
        top_p=0.9
    )
    generated_text = response[0]["generated_text"]
    if "Answer:" in generated_text:
        generated_text = generated_text.split("Answer:", 1)[-1].strip()
    return generated_text

def collect_feedback(response, user_rating):
    feedback_data = {"response": response, "user_rating": user_rating}
    with open("feedback.json", "a") as f:
        json.dump(feedback_data, f)
        f.write("\n")

# Streamlit App
st.title("Empathetic Chatbot")
st.write("This chatbot detects your emotions and provides an empathetic response to your query.")

query = st.text_input("How are you feeling today?", placeholder="Type your thoughts here...")

if st.button("Get Response"):
    if query:
        # Step 1: Emotion Detection
        emotion = get_emotion(query)
        st.write(f"**Detected Emotion:** {emotion}")

        # Step 2: Context Retrieval
        context = retrieve_context(query, emotion, top_k=1)
        st.write("**Relevant Context Retrieved:**", context)

        # Step 3: Generate Response
        response = generate_response(context, query, emotion)
        st.write("**Chatbot Response:**")
        st.success(response)

        # Feedback Section
        st.write("### Provide Feedback")
        user_rating = st.slider("Rate the response (1 - 5):", 1, 5, value=3)
        if st.button("Submit Feedback"):
            collect_feedback(response, user_rating)
            st.success("Thank you for your feedback!")
    else:
        st.warning("Please enter a query to proceed.")

